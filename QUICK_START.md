# Quick Start Guide - Model Persistence & <2s Performance

## âœ… Model Persistence (Already Working!)

**Your model is already saved!** Ollama automatically saves models locally after downloading.

### Verify Your Model is Saved

```bash
ollama list
```

You should see:
```
llama3.2:1b    baf6a787fdff    1.3 GB    [recent date]
```

**âœ… No need to download again!** The model stays saved permanently.

## ðŸš€ Achieving <2 Second Performance

### Method 1: Use Startup Script (Easiest - Recommended)

```bash
./start.sh
```

This automatically:
- âœ… Sets `OLLAMA_KEEP_ALIVE=5m` (keeps model in memory)
- âœ… Checks if Ollama is running
- âœ… Starts Ollama if needed
- âœ… Runs your pipeline with optimal settings

### Method 2: Set Keep-Alive Manually

```bash
# Set keep-alive (keeps model in memory for 5 minutes)
export OLLAMA_KEEP_ALIVE=5m

# Run your script
python main.py
```

**To make it permanent:**
```bash
# Add to your shell profile
echo 'export OLLAMA_KEEP_ALIVE=5m' >> ~/.zshrc
source ~/.zshrc
```

### Method 3: Pre-warm Before Processing

```bash
# Pre-warm the model (loads into memory)
python optimize_for_speed.py

# Then run your pipeline
python main.py
```

## ðŸ“Š Performance Expectations

### Without Optimization
- First call: 3-5 seconds (cold start)
- Subsequent calls: 1.5-3 seconds

### With Optimization (Keep-Alive + Pre-warming)
- First call: 2-3 seconds (pre-warming)
- Subsequent calls: **0.5-1.5 seconds** âœ…
- New OCR data: **<2 seconds** âœ…

### Database Lookup
- Always: **<10ms** (instant) âœ…

## ðŸŽ¯ Workflow for New OCR Data

### Recommended Workflow

1. **Startup** (once per session):
   ```bash
   ./start.sh
   ```
   OR
   ```bash
   export OLLAMA_KEEP_ALIVE=5m
   python main.py
   ```

2. **Process Your Data**:
   - Database lookups: Instant (<10ms)
   - New extractions: <2 seconds (model in memory)
   - Results saved to database for future instant lookups

3. **Subsequent Runs**:
   - Same data: Instant (from database)
   - New data: <2 seconds (model still in memory)

## ðŸ”§ One-Time Setup

Run this once to verify everything is set up:

```bash
python setup_model.py
```

This will:
- âœ… Check if model is downloaded (it is!)
- âœ… Pre-warm the model
- âœ… Provide optimization instructions

## ðŸ“ Key Points

1. **Model Persistence**: âœ… Models are saved locally (no re-download needed)
2. **Performance**: âœ… Use keep-alive or pre-warming for <2s performance
3. **New Data**: âœ… Works with any new OCR data (not just training data)
4. **Database**: âœ… Results are cached for instant future lookups

## ðŸŽ“ Understanding "Training" vs Optimization

### âŒ Model Training (Not What We Do)
- Models are **pre-trained** by Meta
- Training requires large datasets and GPUs
- Training takes hours/days
- **We don't train the model**

### âœ… Inference Optimization (What We Do)
- Optimize **how we use** the pre-trained model
- Pre-warm model into memory
- Use keep-alive to prevent unloading
- Limit output tokens for faster generation
- Optimize sampling parameters

**Result**: Consistent <2 second performance with any new OCR data!

## ðŸš€ Quick Commands

```bash
# Check if model is saved
ollama list

# Setup (one-time)
python setup_model.py

# Run with optimal performance
./start.sh

# OR manually
export OLLAMA_KEEP_ALIVE=5m
python main.py

# Test performance
python test_performance.py

# Optimize for speed
python optimize_for_speed.py
```

## âœ… Summary

1. âœ… **Model is saved** - No need to download again
2. âœ… **Use `./start.sh`** - Easiest way to get <2s performance
3. âœ… **Set keep-alive** - Keeps model in memory for fast inference
4. âœ… **Works with new data** - Not limited to training data
5. âœ… **Database caching** - Instant lookups for repeated data

Your system is ready for fast, consistent performance! ðŸŽ‰

